<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Web Audio API Experiment</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 50px auto;
            padding: 20px;
            background: #f5f5f5;
        }
        h1 {
            color: #333;
        }
        .container {
            background: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        input[type="file"] {
            margin: 20px 0;
        }
        button {
            background: #007bff;
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 16px;
        }
        button:hover {
            background: #0056b3;
        }
        button:disabled {
            background: #ccc;
            cursor: not-allowed;
        }
        #results {
            margin-top: 20px;
            padding: 15px;
            background: #f8f9fa;
            border-radius: 4px;
        }
        canvas {
            width: 100%;
            height: 200px;
            border: 1px solid #ddd;
            margin-top: 10px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Web Audio API - Basic Experiment</h1>
        <p>This is a simple experiment to test Web Audio API capabilities.</p>

        <input type="file" id="audioFile" accept="audio/*">
        <button id="analyzeBtn" disabled>Analyze Audio</button>

        <div id="results"></div>
        <canvas id="waveform"></canvas>
    </div>

    <script>
        const fileInput = document.getElementById('audioFile');
        const analyzeBtn = document.getElementById('analyzeBtn');
        const results = document.getElementById('results');
        const canvas = document.getElementById('waveform');
        const ctx = canvas.getContext('2d');

        let audioContext;
        let audioBuffer;

        fileInput.addEventListener('change', (e) => {
            if (e.target.files.length > 0) {
                analyzeBtn.disabled = false;
            }
        });

        analyzeBtn.addEventListener('click', async () => {
            const file = fileInput.files[0];
            if (!file) return;

            results.innerHTML = '<p>Processing...</p>';

            try {
                // Create AudioContext
                if (!audioContext) {
                    audioContext = new AudioContext();
                }

                // Read and decode audio file
                const arrayBuffer = await file.arrayBuffer();
                audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

                // Extract basic features
                const duration = audioBuffer.duration;
                const sampleRate = audioBuffer.sampleRate;
                const channels = audioBuffer.numberOfChannels;

                // Get audio data
                const audioData = audioBuffer.getChannelData(0);

                // Calculate simple features
                const rms = calculateRMS(audioData);
                const zcr = calculateZCR(audioData);

                // Display results
                results.innerHTML = `
                    <h3>Audio Features:</h3>
                    <p><strong>Duration:</strong> ${duration.toFixed(2)} seconds</p>
                    <p><strong>Sample Rate:</strong> ${sampleRate} Hz</p>
                    <p><strong>Channels:</strong> ${channels}</p>
                    <p><strong>RMS Energy:</strong> ${rms.toFixed(6)}</p>
                    <p><strong>Zero Crossing Rate:</strong> ${zcr.toFixed(6)}</p>
                `;

                // Draw waveform
                drawWaveform(audioData);

                console.log('Analysis complete:', {
                    duration,
                    sampleRate,
                    channels,
                    rms,
                    zcr
                });

            } catch (error) {
                results.innerHTML = `<p style="color: red;">Error: ${error.message}</p>`;
                console.error('Error processing audio:', error);
            }
        });

        function calculateRMS(audioData) {
            let sum = 0;
            for (let i = 0; i < audioData.length; i++) {
                sum += audioData[i] * audioData[i];
            }
            return Math.sqrt(sum / audioData.length);
        }

        function calculateZCR(audioData) {
            let crossings = 0;
            for (let i = 1; i < audioData.length; i++) {
                if ((audioData[i] >= 0 && audioData[i - 1] < 0) ||
                    (audioData[i] < 0 && audioData[i - 1] >= 0)) {
                    crossings++;
                }
            }
            return crossings / audioData.length;
        }

        function drawWaveform(audioData) {
            const width = canvas.width = canvas.offsetWidth;
            const height = canvas.height = 200;

            ctx.fillStyle = '#fff';
            ctx.fillRect(0, 0, width, height);

            ctx.strokeStyle = '#007bff';
            ctx.lineWidth = 1;

            const step = Math.ceil(audioData.length / width);
            const amp = height / 2;

            ctx.beginPath();
            for (let i = 0; i < width; i++) {
                const min = audioData.slice(i * step, (i + 1) * step)
                    .reduce((a, b) => Math.min(a, b), 1);
                const max = audioData.slice(i * step, (i + 1) * step)
                    .reduce((a, b) => Math.max(a, b), -1);

                ctx.lineTo(i, (1 + min) * amp);
                ctx.lineTo(i, (1 + max) * amp);
            }
            ctx.stroke();

            // Center line
            ctx.strokeStyle = '#ccc';
            ctx.beginPath();
            ctx.moveTo(0, height / 2);
            ctx.lineTo(width, height / 2);
            ctx.stroke();
        }
    </script>
</body>
</html>
